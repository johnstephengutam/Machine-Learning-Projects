{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005b8101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johns\\anaconda3\\envs\\cs688\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\johns\\anaconda3\\envs\\cs688\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\johns\\anaconda3\\envs\\cs688\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\johns\\anaconda3\\envs\\cs688\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\johns\\anaconda3\\envs\\cs688\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iterations: 10000\n",
      "Cross-entropy error on training set: nan\n",
      "Classification error on training data: 1.0\n",
      "Classification error on test data: 1.0\n",
      "Time to train the model: 0.6359665393829346 seconds\n",
      "Max iterations: 100000\n",
      "Cross-entropy error on training set: nan\n",
      "Classification error on training data: 1.0\n",
      "Classification error on test data: 1.0\n",
      "Time to train the model: 6.508753776550293 seconds\n",
      "Max iterations: 1000000\n",
      "Cross-entropy error on training set: nan\n",
      "Classification error on training data: 1.0\n",
      "Classification error on test data: 1.0\n",
      "Time to train the model: 71.25149536132812 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import time\n",
    "\n",
    "# Load the Cleveland dataset\n",
    "url_train = \"cleveland-train.csv\"\n",
    "url_test = \"cleveland-test.csv\"\n",
    "\n",
    "# Load the training dataset\n",
    "df_train = pd.read_csv(url_train)\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = pd.read_csv(url_test)\n",
    "X_test = df_test.iloc[:, :-1].values\n",
    "y_test = df_test.iloc[:, -1].values\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, learning_rate=1e-5, max_iter=1000000, tol=1e-6):\n",
    "    #num_features = X.shape[1]\n",
    "    #num_samples = X.shape[0]\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    t = 0\n",
    "    errors = []\n",
    "    \n",
    "    while t < max_iter:\n",
    "        gradients = float(2/num_samples) * X.T.dot(y - np.exp(X.dot(w)))\n",
    "        if np.max(np.abs(gradients)) < tol:\n",
    "            break\n",
    "        w = w - learning_rate * gradients\n",
    "        t += 1\n",
    "        error = logistic_loss(X, y, w)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return w, errors, classification_error(X_train, y_train, w), classification_error(X_test, y_test, w)\n",
    "\n",
    "# Define the logistic loss function\n",
    "def logistic_loss(X, y, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    y_pred = (z / (1 + z)).round()\n",
    "    return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = predict(X, w)\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(X, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    return (z / (1 + z)).round()\n",
    "\n",
    "# Run experiments\n",
    "for max_iter in [10000, 100000, 1000000]:\n",
    "    start_time = time()\n",
    "    w, errors, train_error, test_error = gradient_descent(X_train, y_train, max_iter=max_iter)\n",
    "    end_time = time()\n",
    "    print(f\"Max iterations: {max_iter}\")\n",
    "    print(f\"Cross-entropy error on training set: {errors[-1]}\")\n",
    "    print(f\"Classification error on training data: {train_error}\")\n",
    "    print(f\"Classification error on test data: {test_error}\")\n",
    "    print(f\"Time to train the model: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import time\n",
    "\n",
    "# Load the Cleveland dataset\n",
    "url_train = \"files/cleveland-train.csv\"\n",
    "url_test = \"files/cleveland-test.csv\"\n",
    "\n",
    "# Load the training dataset\n",
    "df_train = pd.read_csv(url_train)\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = pd.read_csv(url_test)\n",
    "X_test = df_test.iloc[:, :-1].values\n",
    "y_test = df_test.iloc[:, -1].values\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, learning_rate=1e-5, max_iter=1000000, tol=1e-3):\n",
    "    #num_features = X.shape[1]\n",
    "    #num_samples = X.shape[0]\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    #print(\"w value\", w)\n",
    "    #print(\"w shape\", w.shape)\n",
    "    #print(\"y value\", y)\n",
    "    #print(\"y shape\", y.shape)\n",
    "    t = 0\n",
    "    errors = []\n",
    "    \n",
    "    while t < max_iter:\n",
    "        #print (t)\n",
    "        #print(\"X shape\",X.shape)\n",
    "        #print(\"w shape\", w.shape)\n",
    "        #print(\"X.dot(w).shape shape1\", X.dot(w).shape)\n",
    "        #print(\"X.T shape\",X.T.shape)\n",
    "        #print(\"y - np.exp(X.dot(w)) shape\", (y - np.exp(X.dot(w))).shape)\n",
    "        print(\"X.T\",X.T)\n",
    "        print(\"y\", y)\n",
    "        print(\"w\",w)\n",
    "        print(\"np.exp(X.dot(w))\", np.exp(X.dot(w)))\n",
    "        print(\"y - np.exp(X.dot(w))\", (y - np.exp(X.dot(w))))\n",
    "        \n",
    "        \n",
    "        gradients = float(2/num_samples) * X.T.dot(y - np.exp(X.dot(w)))\n",
    "        if np.max(np.abs(gradients)) < tol:\n",
    "            break\n",
    "        w = w - learning_rate * gradients\n",
    "        t += 1\n",
    "        error = logistic_loss(X, y, w)\n",
    "        errors.append(error)\n",
    "    \n",
    "    \n",
    "    return w, errors, classification_error(X_train, y_train, w), classification_error(X_test, y_test, w)\n",
    "\n",
    "# Define the logistic loss function\n",
    "def logistic_loss(X, y, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    y_pred = (z / (1 + z)).round()\n",
    "    return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = predict(X, w)\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(X, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    return (z / (1 + z)).round()\n",
    "\n",
    "# Run experiments\n",
    "for max_iter in [10000]:\n",
    "    start_time = time()\n",
    "    w, errors, train_error, test_error = gradient_descent(X_train, y_train, max_iter=max_iter)\n",
    "    end_time = time()\n",
    "    print(f\"Max iterations: {max_iter}\")\n",
    "    print(f\"Cross-entropy error on training set: {errors[-1]}\")\n",
    "    print(f\"Classification error on training data: {train_error}\")\n",
    "    print(f\"Classification error on test data: {test_error}\")\n",
    "    print(f\"Time to train the model: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import time\n",
    "\n",
    "# Load the Cleveland dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "df = pd.read_csv(url, names=names)\n",
    "\n",
    "# Load the Cleveland dataset\n",
    "#url = \"cleveland-train.csv\"\n",
    "#names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "#df = pd.read_csv(url, names=names)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train\",X_train)\n",
    "print(\"y_train\",y_train)\n",
    "print(\"X_test\",X_test)\n",
    "print(\"y_test\",y_test)\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, learning_rate=1e-5, max_iter=1000000, tol=1e-3):\n",
    "    num_features = X.shape[1]\n",
    "    num_samples = X.shape[0]\n",
    "    w = np.zeros(num_features)\n",
    "    print(\"x.shape \",X)\n",
    "    print(\"w.shape \",w)\n",
    "    t = 0\n",
    "    errors = []\n",
    "    \n",
    "    while t < max_iter:\n",
    "        gradients = 2/num_samples * X.T.dot(y - np.exp(X.dot(w)))\n",
    "        if np.max(np.abs(gradients)) < tol:\n",
    "            break\n",
    "        w = w - learning_rate * gradients\n",
    "        t += 1\n",
    "        error = logistic_loss(X, y, w)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return w, errors, classification_error(X_train, y_train, w), classification_error(X_test, y_test, w)\n",
    "\n",
    "# Define the logistic loss function\n",
    "def logistic_loss(X, y, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    y_pred = (z / (1 + z)).round()\n",
    "    return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = predict(X, w)\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(X, w):\n",
    "    z = np.exp(X.dot(w))\n",
    "    return (z / (1 + z)).round()\n",
    "\n",
    "# Run experiments\n",
    "for max_iter in [10000, 100000, 1000000]:\n",
    "    start_time = time()\n",
    "    w, errors, train_error, test_error = gradient_descent(X_train, y_train, max_iter=max_iter)\n",
    "    end_time = time()\n",
    "    print(f\"Max iterations: {max_iter}\")\n",
    "    print(f\"Cross-entropy error on training set: {errors[-1]}\")\n",
    "    print(f\"Classification error on training data: {train_error}\")\n",
    "    print(f\"Classification error on test data: {test_error}\")\n",
    "    print(f\"Time to train the model: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
