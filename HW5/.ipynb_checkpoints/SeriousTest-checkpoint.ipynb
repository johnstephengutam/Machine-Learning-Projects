{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81672617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def greedy_bandit(probs, num_iterations):\n",
    "    num_actions = len(probs)\n",
    "    rewards = np.zeros(num_actions)\n",
    "    counts = np.zeros(num_actions)\n",
    "\n",
    "    total_reward = 0\n",
    "    regret = []\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        action = np.argmax(rewards)\n",
    "        reward = np.random.binomial(1, probs[action])\n",
    "        total_reward += reward\n",
    "        regret.append(max(probs) - probs[action])\n",
    "\n",
    "        counts[action] += 1\n",
    "        rewards[action] += (reward - rewards[action]) / counts[action]\n",
    "\n",
    "    return total_reward, np.cumsum(regret) / np.arange(1, num_iterations + 1)\n",
    "\n",
    "def ucb1_bandit(probs, num_iterations):\n",
    "    num_actions = len(probs)\n",
    "    rewards = np.zeros(num_actions)\n",
    "    counts = np.zeros(num_actions)\n",
    "\n",
    "    total_reward = 0\n",
    "    regret = []\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        ucb_values = rewards + np.sqrt(2 * np.log(t + 1) / (counts + 1e-6))\n",
    "        action = np.argmax(ucb_values)\n",
    "        reward = np.random.binomial(1, probs[action])\n",
    "        total_reward += reward\n",
    "        regret.append(max(probs) - probs[action])\n",
    "\n",
    "        counts[action] += 1\n",
    "        rewards[action] += (reward - rewards[action]) / counts[action]\n",
    "\n",
    "    return total_reward, np.cumsum(regret) / np.arange(1, num_iterations + 1)\n",
    "\n",
    "def thompson_sampling_bandit(probs, num_iterations):\n",
    "    num_actions = len(probs)\n",
    "    successes = np.zeros(num_actions)\n",
    "    failures = np.zeros(num_actions)\n",
    "\n",
    "    total_reward = 0\n",
    "    regret = []\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        sampled_theta = np.random.beta(successes + 1, failures + 1)\n",
    "        action = np.argmax(sampled_theta)\n",
    "        reward = np.random.binomial(1, probs[action])\n",
    "        total_reward += reward\n",
    "        regret.append(max(probs) - probs[action])\n",
    "\n",
    "        if reward == 1:\n",
    "            successes[action] += 1\n",
    "        else:\n",
    "            failures[action] += 1\n",
    "\n",
    "    return total_reward, np.cumsum(regret) / np.arange(1, num_iterations + 1)\n",
    "\n",
    "def test_algorithm(algorithm, probs, num_iterations, num_trials):\n",
    "    total_rewards = np.zeros(num_trials)\n",
    "    avg_regrets = np.zeros(num_iterations)\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        total_reward, regrets = algorithm(probs, num_iterations)\n",
    "        total_rewards[i] = total_reward\n",
    "        avg_regrets += (regrets - avg_regrets) / (i + 1)\n",
    "\n",
    "    return total_rewards.mean(), avg_regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933b7810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Eleven-armed bandit:\n",
      "Algorithm: greedy_bandit\n",
      "Iterations: 1000, Mean reward: 0.0, Mean regret: 1.0\n",
      "Iterations: 10000, Mean reward: 0.0, Mean regret: 1.0\n",
      "Iterations: 100000, Mean reward: 0.0, Mean regret: 1.0\n",
      "Algorithm: ucb1_bandit\n",
      "Iterations: 1000, Mean reward: 846.12, Mean regret: 0.15328999999999926\n",
      "Iterations: 10000, Mean reward: 9634.36, Mean regret: 0.03638260000000045\n",
      "Iterations: 100000, Mean reward: 99418.14, Mean regret: 0.005800630000000247\n",
      "Algorithm: thompson_sampling_bandit\n",
      "Iterations: 1000, Mean reward: 986.08, Mean regret: 0.014254999999999999\n",
      "Iterations: 10000, Mean reward: 9986.1, Mean regret: 0.0013903\n",
      "Iterations: 100000, Mean reward: 99986.17, Mean regret: 0.0001377999999999999\n",
      "Testing on Five-armed bandit:\n",
      "Algorithm: greedy_bandit\n",
      "Iterations: 1000, Mean reward: 299.48, Mean regret: 0.5500000000000044\n",
      "Iterations: 10000, Mean reward: 3007.03, Mean regret: 0.5500000000001022\n",
      "Iterations: 100000, Mean reward: 29998.07, Mean regret: 0.5500000000008705\n",
      "Algorithm: ucb1_bandit\n",
      "Iterations: 1000, Mean reward: 790.84, Mean regret: 0.05796530000000053\n",
      "Iterations: 10000, Mean reward: 8298.44, Mean regret: 0.01960542000000157\n",
      "Iterations: 100000, Mean reward: 84448.09, Mean regret: 0.005631584999998115\n",
      "Algorithm: thompson_sampling_bandit\n",
      "Iterations: 1000, Mean reward: 830.4, Mean regret: 0.019289899999999863\n",
      "Iterations: 10000, Mean reward: 8453.91, Mean regret: 0.004518010000000033\n",
      "Iterations: 100000, Mean reward: 84918.16, Mean regret: 0.0008003910000000199\n"
     ]
    }
   ],
   "source": [
    "# Test settings\n",
    "bandit_settings = [\n",
    "    (np.arange(0, 1.1, 0.1), \"Eleven-armed bandit\"),\n",
    "    ([0.3, 0.5, 0.7, 0.83, 0.85], \"Five-armed bandit\")\n",
    "]\n",
    "algorithms = [greedy_bandit, ucb1_bandit, thompson_sampling_bandit]\n",
    "num_iterations = [10**3, 10**4, 10**5]\n",
    "num_trials = 100\n",
    "\n",
    "# Run experiments\n",
    "for probs, setting_name in bandit_settings:\n",
    "    print(f\"Testing on {setting_name}:\")\n",
    "    for algorithm in algorithms:\n",
    "        print(f\"Algorithm: {algorithm.__name__}\")\n",
    "        for iteration in num_iterations:\n",
    "            total_reward_mean, avg_regrets = test_algorithm(algorithm, probs, iteration, num_trials)\n",
    "            print(f\"Iterations: {iteration}, Mean reward: {total_reward_mean}, Mean regret: {avg_regrets[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455df775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(algorithms, num_iterations, num_trials, bandit_settings):\n",
    "    for probs, setting_name in bandit_settings:\n",
    "        print(f\"Testing on {setting_name}:\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for algorithm in algorithms:\n",
    "            print(f\"Algorithm: {algorithm.__name__}\")\n",
    "            avg_regrets_over_iterations = np.zeros((len(num_iterations), num_iterations[-1]))\n",
    "            for i, iteration in enumerate(num_iterations):\n",
    "                _, avg_regrets = test_algorithm(algorithm, probs, iteration, num_trials)\n",
    "                avg_regrets_over_iterations[i, :iteration] = avg_regrets\n",
    "            avg_regrets_mean = np.mean(avg_regrets_over_iterations, axis=0)\n",
    "            plt.plot(np.arange(1, num_iterations[-1] + 1), avg_regrets_mean, label=algorithm.__name__)\n",
    "        plt.title(f\"Average Regret vs. Time for {setting_name}\")\n",
    "        plt.xlabel(\"Time steps\")\n",
    "        plt.ylabel(\"Average Regret\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize action selection over time\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for algorithm in algorithms:\n",
    "            action_selection_counts = np.zeros((len(probs), num_iterations[-1]))\n",
    "            for _ in range(num_trials):\n",
    "                action_selection = np.zeros(num_iterations[-1])\n",
    "                for t in range(num_iterations[-1]):\n",
    "                    action = algorithm(probs, num_iterations[-1])[0]\n",
    "                    action_selection[t] = action\n",
    "                for a in range(len(probs)):\n",
    "                    action_selection_counts[a] += (action_selection == a).astype(int)\n",
    "            avg_action_selection_counts = action_selection_counts / num_trials\n",
    "            for a in range(len(probs)):\n",
    "                plt.plot(np.arange(1, num_iterations[-1] + 1), avg_action_selection_counts[a], label=f\"Action {a + 1} - {algorithm.__name__}\")\n",
    "        plt.title(f\"Action Selection Over Time for {setting_name}\")\n",
    "        plt.xlabel(\"Time steps\")\n",
    "        plt.ylabel(\"Probability of Selection\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Run experiments and plot results\n",
    "plot_results(algorithms, num_iterations, num_trials, bandit_settings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
